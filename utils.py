import time
import requests
import json
from typing import Tuple, Dict, List
from xiaohongshu_model import Xiaohongshu
from prompt_template import system_template_text, user_template_text
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI
import dashscope
from character_templates import CHARACTER_TEMPLATES
from api_clients import create_client
import io
from docx import Document
import PyPDF2
import streamlit as st


def create_copy_button(text: str, button_text: str = "ğŸ“‹ å¤åˆ¶åˆ°å‰ªè´´æ¿", key: str = None) -> None:
    """ä½¿ç”¨ Streamlit åŸç”Ÿç»„ä»¶åˆ›å»ºå¤åˆ¶æŒ‰é’®"""
    if key not in st.session_state:
        st.session_state[key] = False

    if st.button(button_text, key=f"btn_{key}", use_container_width=True):
        try:
            import pyperclip
            pyperclip.copy(text)
            st.session_state[key] = True
            st.success('âœ… å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼', icon="âœ…")
        except ImportError:
            st.error('è¯·å…ˆå®‰è£… pyperclip: pip install pyperclip')

def verify_api_key(model_type: str, api_key: str, max_retries: int = 2) -> Tuple[bool, str]:
    """éªŒè¯APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    print(f"å¼€å§‹éªŒè¯å¯†é’¥: model_type={model_type}")

    for attempt in range(max_retries):
        try:
            test_prompt = "Hi"
            print(f"åˆ›å»ºå®¢æˆ·ç«¯: ç¬¬{attempt + 1}æ¬¡å°è¯•")
            client = create_client(model_type, api_key, temperature=0.1)
            print("å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸï¼Œå‘é€æµ‹è¯•è¯·æ±‚")
            response = client.chat(test_prompt)
            print(f"éªŒè¯æˆåŠŸ: {response[:100]}")
            return True, "APIå¯†é’¥éªŒè¯æˆåŠŸ"
        except Exception as e:
            print(f"éªŒè¯é”™è¯¯: {str(e)}")
            if attempt == max_retries - 1:
                error_msg = str(e)
                if "auth" in error_msg.lower():
                    return False, "APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ"
                elif "rate" in error_msg.lower():
                    return False, "APIè°ƒç”¨é¢‘ç‡è¶…é™"
                else:
                    return False, f"éªŒè¯å¤±è´¥: {error_msg}"
            time.sleep(1)
    return False, "éªŒè¯è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"


def generate_script(subject: str, video_length: float, creativity: float,
                    model_type: str, api_key: str, temperature: float = 0.2) -> Tuple[str, str]:
    """ç»Ÿä¸€çš„è„šæœ¬ç”Ÿæˆå‡½æ•°

    Args:
        subject: è§†é¢‘ä¸»é¢˜
        video_length: è§†é¢‘æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰
        creativity: åˆ›æ„åº¦
        model_type: ä½¿ç”¨çš„æ¨¡å‹ç±»å‹
        api_key: APIå¯†é’¥
        temperature: æ¸©åº¦å‚æ•°

    Returns:
        Tuple[str, str]: (æ ‡é¢˜, è„šæœ¬å†…å®¹)
    """
    try:
        client = create_client(model_type, api_key, temperature)

        # ç”Ÿæˆæ ‡é¢˜çš„æ¨¡æ¿
        title_template = f"è¯·ä¸º'{subject}'è¿™ä¸ªä¸»é¢˜çš„è§†é¢‘æƒ³ä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜ï¼Œç›´æ¥è¾“å‡ºæ ‡é¢˜å³å¯ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹å’Œè§£é‡Šã€‚"

        # ç”Ÿæˆè„šæœ¬çš„æ¨¡æ¿
        script_template = f"""ä½ æ˜¯ä¸€ä½çŸ­è§†é¢‘å†…å®¹åˆ›ä½œä¸“å®¶ã€‚æ ¹æ®ä»¥ä¸‹è¦æ±‚ï¼Œä¸ºçŸ­è§†é¢‘åˆ›ä½œä¸€ä¸ªè¯¦ç»†çš„è„šæœ¬ã€‚

ä¸»é¢˜ï¼š{subject}
æ—¶é•¿ï¼š{video_length}åˆ†é’Ÿ
åˆ›æ„åº¦ï¼š{creativity}ï¼ˆ0-1ä¹‹é—´ï¼Œè¶Šå¤§åˆ›æ„æ€§è¶Šå¼ºï¼‰

è¦æ±‚ï¼š
1. æ•´ä½“å†…å®¹é•¿åº¦è¦ç¬¦åˆè§†é¢‘æ—¶é•¿çš„è¦æ±‚
2. è„šæœ¬åˆ†ä¸ºã€å¼€å¤´ã€‘ã€ä¸­é—´ã€‘ã€ç»“å°¾ã€‘ä¸‰éƒ¨åˆ†
3. å¼€å¤´è¦å¸å¼•çœ¼çƒï¼Œå¿«é€ŸæŠ“ä½è§‚ä¼—æ³¨æ„åŠ›
4. ä¸­é—´éƒ¨åˆ†è¦æœ‰å¹²è´§å†…å®¹ï¼Œæ³¨æ„èŠ‚å¥æ„Ÿ
5. ç»“å°¾è¦æœ‰æƒŠå–œæˆ–æ„æ–™ä¹‹å¤–çš„è½¬æŠ˜
6. æ•´ä½“è¡¨è¾¾è¦è½»æ¾æœ‰è¶£ï¼Œé€‚åˆå¹´è½»äººè§‚çœ‹
7. å¯ä»¥åŠ å…¥ä¸€äº›æµè¡Œæ¢—æˆ–è€…æœ‰è¶£çš„å…ƒç´ 

è¯·ç›´æ¥ç»™å‡ºè„šæœ¬å†…å®¹ï¼ŒæŒ‰ç…§ã€å¼€å¤´ã€‘ã€ä¸­é—´ã€‘ã€ç»“å°¾ã€‘çš„æ ¼å¼åˆ†æ®µè¾“å‡ºã€‚"""

        # ç”Ÿæˆæ ‡é¢˜
        title_response = client.chat(title_template)
        title = title_response.strip()

        # ç”Ÿæˆè„šæœ¬
        script_prompt = script_template
        script_response = client.chat(script_prompt)
        script = script_response.strip()

        return title, script

    except Exception as e:
        raise Exception(f"è„šæœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")


def generate_xiaohongshu_content(theme: str, model_type: str, api_key: str, temperature: float = 0.2) -> dict:
    """ç”Ÿæˆå°çº¢ä¹¦å†…å®¹çš„å‡½æ•°"""
    try:
        client = create_client(model_type, api_key, temperature)

        full_prompt = f"""{system_template_text}

{user_template_text.format(theme=theme)}

è¯·ç›´æ¥æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºå†…å®¹ï¼Œä¸è¦è¾“å‡ºJSONæ ¼å¼ï¼š

[æ ‡é¢˜éƒ¨åˆ†]
æ ‡é¢˜1
æ ‡é¢˜2
æ ‡é¢˜3
æ ‡é¢˜4
æ ‡é¢˜5

[æ­£æ–‡éƒ¨åˆ†]
æ­£æ–‡å†…å®¹...

[æ ‡ç­¾éƒ¨åˆ†]
#æ ‡ç­¾1 #æ ‡ç­¾2 #æ ‡ç­¾3 ..."""

        # è·å–å“åº”
        response = client.chat(full_prompt)

        try:
            # è§£æè¿”å›çš„å†…å®¹
            sections = response.split('[')
            titles = []
            content = ""
            tags = []

            for section in sections:
                if 'æ ‡é¢˜éƒ¨åˆ†]' in section:
                    # æå–æ ‡é¢˜
                    titles_text = section.split(']')[1].strip()
                    titles = [t.strip() for t in titles_text.split('\n') if t.strip()]
                elif 'æ­£æ–‡éƒ¨åˆ†]' in section:
                    # æå–æ­£æ–‡
                    content = section.split(']')[1].strip()
                elif 'æ ‡ç­¾éƒ¨åˆ†]' in section:
                    # æå–æ ‡ç­¾
                    tags_text = section.split(']')[1].strip()
                    tags = [tag.strip('#') for tag in tags_text.split('#') if tag.strip()]

            # éªŒè¯æ•°æ®
            if len(titles) < 5:
                raise ValueError("ç”Ÿæˆçš„æ ‡é¢˜æ•°é‡ä¸è¶³5ä¸ª")

            # éªŒè¯æ•°æ®æ ¼å¼
            Xiaohongshu(
                titles=titles,
                content=content
            )

            # è¿”å›ç»“æœ
            return {
                "title": titles[0],
                "content": content,
                "tags": tags
            }

        except Exception as parse_error:
            raise ValueError(f"è§£æå“åº”å†…å®¹å‡ºé”™: {str(parse_error)}")

    except Exception as e:
        raise Exception(f"å°çº¢ä¹¦å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")


def generate_character_prompt(character_type: str, user_prompt: str) -> str:
    """æ ¹æ®é€‰æ‹©çš„äººè®¾ç”Ÿæˆå®Œæ•´çš„æç¤ºè¯"""
    if character_type not in CHARACTER_TEMPLATES:
        return user_prompt

    character = CHARACTER_TEMPLATES[character_type]
    system_prompt = f"""
{character['personality']}

ä»¥ä¸‹æ˜¯ä¸€äº›ä½ çš„å›å¤ç¤ºä¾‹ï¼Œè¯·å‚è€ƒå…¶ä¸­çš„è¯­æ°”å’Œé£æ ¼ï¼š
{chr(10).join(character['example_responses'])}

è¯·å§‹ç»ˆä¿æŒè¿™ä¸ªäººè®¾é£æ ¼å›å¤ç”¨æˆ·çš„æ¶ˆæ¯ã€‚
ç°åœ¨ç”¨æˆ·è¯´ï¼š{user_prompt}
"""
    return system_prompt


def get_chat_response(prompt: str, memory: ConversationBufferMemory,
                     model_type: str, api_key: str, character_type: str = None,
                     is_chat_feature: bool = False) -> str:
    """Generate chat response with memory support"""
    try:
        # åªæœ‰åœ¨èŠå¤©åŠŸèƒ½ä¸­æ‰ä½¿ç”¨å†å²è®°å¿†å’Œäººè®¾
        if is_chat_feature and memory:
            chat_history = ""
            if memory.chat_memory.messages:
                for message in memory.chat_memory.messages:
                    if hasattr(message, 'content') and message.content:
                        role = 'Human' if message.type == 'human' else 'Assistant'
                        chat_history += f"{role}: {message.content}\n"

            full_prompt = f"""
å†å²å¯¹è¯:
{chat_history}

å½“å‰é—®é¢˜: {prompt}

è¯·åŸºäºä»¥ä¸Šå†å²å¯¹è¯å›ç­”å½“å‰é—®é¢˜ã€‚
"""
            # åªåœ¨èŠå¤©åŠŸèƒ½ä¸­åº”ç”¨äººè®¾
            if character_type:
                full_prompt = generate_character_prompt(character_type, full_prompt)
        else:
            # å…¶ä»–åŠŸèƒ½ç›´æ¥ä½¿ç”¨åŸå§‹prompt
            full_prompt = prompt

        print(f"Using model: {model_type}")

        # æ ¹æ®ä¸åŒæ¨¡å‹è·å–å“åº”
        response = ""
        if model_type == "qwen":
            response = _get_qwen_response(full_prompt, api_key)
        elif model_type == "chatgpt":
            response = _get_chatgpt_response(full_prompt, api_key)
        elif model_type == "claude":
            response = _get_claude_response(full_prompt, api_key)
        elif model_type == "glm":
            response = _get_glm_response(full_prompt, api_key)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

        if not response or response.startswith("API"):
            print(f"Warning: Invalid response: {response}")
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆæœ‰æ•ˆå›å¤ï¼Œè¯·ç¨åå†è¯•ã€‚"

        # åªåœ¨èŠå¤©åŠŸèƒ½ä¸­ä¿å­˜å¯¹è¯è®°å¿†
        if is_chat_feature and memory:
            memory.chat_memory.add_user_message(prompt)
            memory.chat_memory.add_ai_message(response)

        return response

    except Exception as e:
        print(f"Error in get_chat_response: {str(e)}")
        return f"æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"


def _get_qwen_response(prompt: str, api_key: str) -> str:
    """Get response from Qwen API with better error handling"""
    try:
        dashscope.api_key = api_key
        response = dashscope.Generation.call(
            model='qwen-max',
            messages=[{'role': 'user', 'content': prompt}],
            result_format='message'
        )
        print(f"Qwen API Response: {response}")  # æ‰“å°å®Œæ•´å“åº”ç”¨äºè°ƒè¯•

        if response.status_code == 200:
            # ä»æ–°çš„å“åº”æ ¼å¼ä¸­æå–æ–‡æœ¬
            if response.output and response.output.choices:
                message = response.output.choices[0].get('message', {})
                response_text = message.get('content', '')
                if response_text:
                    return response_text

            print("Warning: Could not extract valid response from Qwen API output")
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å¾—åˆ°æœ‰æ•ˆçš„å›å¤ï¼Œè¯·é‡è¯•ã€‚"
        else:
            error_msg = f"APIè°ƒç”¨å¤±è´¥: {response.code}, {response.message}"
            print(f"Qwen API Error: {error_msg}")
            return f"APIè°ƒç”¨å‡ºé”™: {response.message}"
    except Exception as e:
        print(f"Error in Qwen API call: {str(e)}")
        return f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"


def _get_chatgpt_response(prompt: str, api_key: str) -> str:
    """Get response from ChatGPT API with better error handling"""
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "gpt-4",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7
        }
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30  # æ·»åŠ è¶…æ—¶è®¾ç½®
        )
        response.raise_for_status()
        content = response.json()['choices'][0]['message']['content']
        if not content:
            print("Warning: Empty response from ChatGPT API")
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å¾—åˆ°æœ‰æ•ˆçš„å›å¤ï¼Œè¯·é‡è¯•ã€‚"
        return content
    except requests.exceptions.RequestException as e:
        print(f"ChatGPT API Request Error: {str(e)}")
        return f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}"
    except Exception as e:
        print(f"Error in ChatGPT API call: {str(e)}")
        return f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"


def _get_claude_response(prompt: str, api_key: str) -> str:
    """Get response from Claude API with better error handling"""
    try:
        headers = {
            "anthropic-version": "2023-06-01",
            "x-api-key": api_key,
            "content-type": "application/json"
        }
        data = {
            "model": "claude-3-sonnet-20240229",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7
        }
        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers=headers,
            json=data,
            timeout=30
        )
        response.raise_for_status()
        content = response.json()['content'][0]['text']
        if not content:
            print("Warning: Empty response from Claude API")
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å¾—åˆ°æœ‰æ•ˆçš„å›å¤ï¼Œè¯·é‡è¯•ã€‚"
        return content
    except requests.exceptions.RequestException as e:
        print(f"Claude API Request Error: {str(e)}")
        return f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}"
    except Exception as e:
        print(f"Error in Claude API call: {str(e)}")
        return f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"


def _get_glm_response(prompt: str, api_key: str, max_retries: int = 3, timeout: int = 60) -> str:
    """Get response from GLM API with improved error handling and retry mechanism

    Args:
        prompt: The prompt text to send
        api_key: GLM API key
        max_retries: Maximum number of retry attempts
        timeout: Timeout in seconds for the request

    Returns:
        Response text from the API
    """
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    import time

    # åˆ›å»ºä¸€ä¸ªå¸¦é‡è¯•æœºåˆ¶çš„session
    session = requests.Session()
    retries = Retry(
        total=max_retries,
        backoff_factor=1,  # é‡è¯•é—´éš”ä¼šæŒ‰1, 2, 4ç§’å»¶é•¿
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["POST"]
    )
    session.mount('https://', HTTPAdapter(max_retries=retries))

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    data = {
        "model": "glm-4-plus",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7
    }

    for attempt in range(max_retries):
        try:
            response = session.post(
                "https://open.bigmodel.cn/api/paas/v4/chat/completions",
                headers=headers,
                json=data,
                timeout=timeout  # å¢åŠ è¶…æ—¶æ—¶é—´
            )

            response.raise_for_status()
            content = response.json()['choices'][0]['message']['content']

            if not content:
                print("Warning: Empty response from GLM API")
                return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å¾—åˆ°æœ‰æ•ˆçš„å›å¤ï¼Œè¯·é‡è¯•ã€‚"
            return content

        except requests.exceptions.Timeout:
            if attempt == max_retries - 1:
                print(f"GLM API final timeout after {max_retries} attempts")
                return f"APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚å»ºè®®ï¼š\n1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n2. å°è¯•ç¼©çŸ­è¾“å…¥å†…å®¹\n3. å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥é€‰æ‹©å…¶ä»–AIæ¨¡å‹"

            print(f"GLM API timeout on attempt {attempt + 1}, retrying...")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                print(f"GLM API Request Error: {str(e)}")
                return f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}"

            print(f"GLM API error on attempt {attempt + 1}, retrying... Error: {str(e)}")
            time.sleep(2 ** attempt)

        except Exception as e:
            print(f"Error in GLM API call: {str(e)}")
            return f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"

    return "è¯·æ±‚å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"


def extract_text_from_pdf(file_content: bytes) -> str:
    """ä»PDFæ–‡ä»¶å†…å®¹ä¸­æå–æ–‡æœ¬"""
    try:
        pdf_file = io.BytesIO(file_content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)

        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"

        return text
    except Exception as e:
        raise Exception(f"PDFæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")


def extract_text_from_docx(file_content: bytes) -> str:
    """ä»DOCXæ–‡ä»¶å†…å®¹ä¸­æå–æ–‡æœ¬"""
    try:
        doc = Document(io.BytesIO(file_content))
        text = []
        for paragraph in doc.paragraphs:
            text.append(paragraph.text)
        return '\n'.join(text)
    except Exception as e:
        raise Exception(f"Wordæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")

def extract_text_from_image(image_content: bytes, api_key: str) -> str:
    """ä»å›¾ç‰‡ä¸­æå–æ–‡å­—å†…å®¹ï¼Œä½¿ç”¨GLM-4V-Flashæ¨¡å‹"""
    import base64
    import requests

    try:
        # å°†å›¾ç‰‡å†…å®¹è½¬æ¢ä¸ºbase64
        image_base64 = base64.b64encode(image_content).decode('utf-8')

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": "glm-4v-flash",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "è¯·æå–å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ï¼Œä¿æŒåŸæœ‰æ ¼å¼ã€‚"
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            }
                        }
                    ]
                }
            ]
        }

        response = requests.post(
            "https://open.bigmodel.cn/api/paas/v4/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )

        response.raise_for_status()
        extracted_text = response.json()['choices'][0]['message']['content']
        return extracted_text

    except Exception as e:
        raise Exception(f"å›¾ç‰‡æ–‡å­—æå–å¤±è´¥: {str(e)}")


def analyze_legal_document(text: str, document_type: str, model_type: str, api_key: str) -> Dict:
    """åˆ†ææ³•å¾‹æ–‡æ¡£å†…å®¹"""
    from utils import _get_glm_response

    # æ ¹æ®æ–‡æ¡£ç±»å‹æ„å»ºä¸åŒçš„æç¤ºè¯
    if document_type == "contract":
        prompt = f"""ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹é¡¾é—®,è¯·å¯¹ä»¥ä¸‹åˆåŒè¿›è¡Œå…¨é¢åˆ†æ:
{text}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œè¯¦ç»†åˆ†æ:
1. åˆåŒä¸»è¦æ¡æ¬¾è§£æ
2. æ½œåœ¨é£é™©ç‚¹å’Œæ³•å¾‹æ¼æ´
3. æ¨¡ç³Šæˆ–æœ‰äº‰è®®çš„æ¡æ¬¾
4. å¯¹ç”²ä¹™åŒæ–¹æƒè´£çš„è¯„ä¼°
5. å…·ä½“çš„ä¿®æ”¹å»ºè®®

è¯·æŒ‰ä»¥ä¸Šé¡ºåºè¿›è¡Œåˆ†æ,å¹¶é‡ç‚¹æ ‡æ³¨éœ€è¦æ³¨æ„çš„å†…å®¹ã€‚"""

    elif document_type == "legal_document":
        prompt = f"""ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹é¡¾é—®,è¯·å¯¹ä»¥ä¸‹æ³•å¾‹æ–‡ä¹¦è¿›è¡Œåˆæ³•æ€§åˆ†æ:
{text}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œè¯¦ç»†åˆ†æ:
1. æ–‡ä¹¦æ ¼å¼è§„èŒƒæ€§
2. æ³•å¾‹ä¾æ®çš„å‡†ç¡®æ€§
3. ç¨‹åºåˆæ³•æ€§
4. å†…å®¹çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
5. å­˜åœ¨çš„é—®é¢˜å’Œæ”¹è¿›å»ºè®®

è¯·æŒ‰ä»¥ä¸Šé¡ºåºè¿›è¡Œåˆ†æ,å¹¶æŒ‡å‡ºéœ€è¦ç‰¹åˆ«æ³¨æ„æˆ–ä¿®æ”¹çš„åœ°æ–¹ã€‚"""

    # è·å–AIåˆ†æç»“æœ
    try:
        analysis = _get_glm_response(prompt, api_key)

        return {
            'status': 'success',
            'analysis': analysis
        }
    except Exception as e:
        return {
            'status': 'error',
            'message': f"åˆ†æå¤±è´¥: {str(e)}"
        }


def get_legal_advice(case_description: str, question: str, model_type: str, api_key: str) -> Dict:
    """è·å–æ³•å¾‹å»ºè®®"""
    from utils import _get_glm_response

    prompt = f"""ä½œä¸ºä¸€ä¸ªä¸“ä¸šå¾‹å¸ˆ,è¯·é’ˆå¯¹ä»¥ä¸‹æ¡ˆä¾‹å’Œé—®é¢˜æä¾›ä¸“ä¸šçš„æ³•å¾‹æ„è§:

æ¡ˆä¾‹æè¿°:
{case_description}

å’¨è¯¢é—®é¢˜:
{question}

è¯·ä»ä»¥ä¸‹æ–¹é¢æä¾›è¯¦ç»†åˆ†æå’Œå»ºè®®:
1. æ³•å¾‹å®šæ€§åˆ†æ
2. ç›¸å…³æ³•å¾‹æ³•è§„
3. å¯èƒ½çš„æ³•å¾‹åæœ
4. è§£å†³æ–¹æ¡ˆå»ºè®®
5. é£é™©æç¤º

è¯·æä¾›ä¸“ä¸šã€å®¢è§‚çš„åˆ†æå’Œå»ºè®®ã€‚"""

    try:
        advice = _get_glm_response(prompt, api_key)

        return {
            'status': 'success',
            'advice': advice
        }
    except Exception as e:
        return {
            'status': 'error',
            'message': f"è·å–å»ºè®®å¤±è´¥: {str(e)}"
        }


def analyze_legal_risk(scenario: str, model_type: str, api_key: str) -> Dict:
    """åˆ†ææ³•å¾‹é£é™©"""
    from utils import _get_glm_response

    prompt = f"""ä½œä¸ºä¸€ä¸ªä¸“ä¸šå¾‹å¸ˆ,è¯·å¯¹ä»¥ä¸‹æƒ…å†µè¿›è¡Œæ³•å¾‹é£é™©åˆ†æ:
{scenario}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ†æ:
1. å¯èƒ½æ¶‰åŠçš„è¿æ³•è¡Œä¸º
2. ç›¸å…³æ³•å¾‹æ³•è§„
3. å¯èƒ½æ‰¿æ‹…çš„æ³•å¾‹è´£ä»»
4. æ½œåœ¨çš„å¤„ç½šåæœ
5. é£é™©é˜²èŒƒå»ºè®®

è¯·è¯¦ç»†è¯´æ˜æ¯ä¸ªæ–¹é¢,å¹¶æä¾›å…·ä½“çš„æ³•å¾‹ä¾æ®ã€‚"""

    try:
        risk_analysis = _get_glm_response(prompt, api_key)

        return {
            'status': 'success',
            'analysis': risk_analysis
        }
    except Exception as e:
        return {
            'status': 'error',
            'message': f"é£é™©åˆ†æå¤±è´¥: {str(e)}"
        }